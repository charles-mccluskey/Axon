namespace nnmodel2;

use Persistence.ump;

class NeuralNetwork{
	Integer mutationRate = 10;

	public  NeuralNetwork(int numInputs, int numHiddenLayers, int nodesPerLayer, int numOutputs, double learningRate){
		mutationRate = 10;
		Random rng = new Random();
	   	layers = new ArrayList<Layer>();
    	//build input layer
    	addLayer();
    	for(int i=0;i<numInputs;i++) {
    		getLayer(0).addNeuron(rng.nextDouble(), 0, 0);
    	}
    	//build hidden layers
    	for(int i=1;i<=numHiddenLayers;i++) {
    		addLayer();
    		for(int j=0;j<nodesPerLayer;j++) {
    			getLayer(i).addNeuron(rng.nextDouble(), 0, 0);
    		}
    	}
    	//build output layer
    	addLayer();
    	for(int i=0;i<numOutputs;i++) {
    		getLayer(1+numHiddenLayers).addNeuron(rng.nextDouble(), 0, 0);
    	}
    	//neurons have been created, now to connect them.
    	for(int l=0;l<=numHiddenLayers;l++) {
    		for(int n=0;n<getLayer(l).numberOfNeurons();n++) {//nodes in neural layer
    			for(int m=0;m<getLayer(l+1).numberOfNeurons();m++) {//nodes in adjacent neural layer
    				Connection con = new Connection(rng.nextDouble(),0,getLayer(l).getNeuron(n),getLayer(l+1).getNeuron(m));// randomly initialize weights
    			}
    		}
    	}
    	setLearningRate(learningRate);
	}
}

class Neuron{
	Integer maxFunctions = 5;
	Integer currentFunction = 0;
	
	private double swish(double input){
		return input * sigmoid(input);
	}
	
	private double swishPrime(double input){
		return Math.exp(input)*(input + Math.exp(input)+1)/Math.pow((Math.exp(input)+1), 2);
	}
	
	private double leakyRelu(double input) {
	  	if(input <=0) {
		  	return input*-0.1;
	  	}else {
		  	return input;
	  	}
  	}
  
  	private double leakyReluPrime(double input) {
	  	if(input <= 0) {
		  	return -0.1;
	  	}else {
		  	return 1;
	  	}
 	}
	
	private double sigmoid(double input) {
		  return 1 / (1 + Math.exp(-1*input));
	}
	
	private double sigPrime(double input) {
  		return sigmoid(input) * (1- sigmoid(input));
  	}
  	
  	private double tanH(double input) {
		return Math.tanh(input);
   	}
   
  	private double tanHPrime(double input) {
		return (1-Math.pow(Math.tanh(input), 2));
  	}
  	
 	private double softPlus(double input) {
		return Math.log(1+Math.exp(input));
   	}
   
   	private double softPlusPrime(double input) {
	   	return 1/(1+Math.exp(-1*input));
   	}
  	
	public void processInputs(){
		double sum = getBias();
		List<Connection> connections = getInputConnections();
		for(int i=0;i<connections.size();i++) {
			sum += connections.get(i).getInputNeuron().getActivation()*connections.get(i).getWeight().getValue(); 
		}
		setActivation(activationFunction(sum));
	}

   public double getInput(){
	   double sum = getBias();
	   List<Connection> connections = getInputConnections();
	   for(int i=0;i<connections.size();i++) {
		   sum += connections.get(i).getInputNeuron().getActivation()*connections.get(i).getWeight().getValue(); 
	   }
	   return sum;
   }
   
   public double sumErrors(){
    if(numberOfOutputConnections() == 0) {
		   return error;
	   }
	   double sum = 0;
	   for(int i=0;i<numberOfOutputConnections();i++) {
		   sum += getOutputConnection(i).getOutputNeuron().getError()
				   * getOutputConnection(i).getWeight().getValue()
				   * activationFunctionDerivative(getOutputConnection(i).getOutputNeuron().getInput());
	   }
	   this.setError(sum);
	   return sum;
  }
 	  
   public double activationFunction(double input){
    if(getCurrentFunction()==0) {
		  return sigmoid(input);
	  }else if(getCurrentFunction()==1) {
		  return tanH(input);
	  }else if(getCurrentFunction()==2) {
		  return softPlus(input);
	  }else if(getCurrentFunction()==3) {
		  return leakyRelu(input);
	  }else if(getCurrentFunction()==4) {
		  return swish(input);
	  }else{
		  return sigmoid(input);
	  }
  }
  
   public double activationFunctionDerivative(double input){
    if(getCurrentFunction()==0) {
		  return sigPrime(input);
	  }else if(getCurrentFunction()==1){
		  return tanHPrime(input);
	  }else if(getCurrentFunction()==2){
		  return softPlusPrime(input);
	  }else if(getCurrentFunction()==3) {
		  return leakyReluPrime(input);
	  }else if(getCurrentFunction()==4) {
		  return swishPrime(input);
	  }else{
		  return sigPrime(input);
	  }
  }
}

class Connection{
  	public void updateWeight() {
  		weight.setValue(weight.getValue()+weight.getChange());
  		weight.setChange(0);
  	}
}